{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LDeMcyMAMhhj"},"outputs":[],"source":["\"\"\"\n","Keras implementation for Deep Embedded Clustering (DEC) algorithm:\n","        Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. ICML 2016.\n","Usage:\n","    Weights of Pretrained autoencoder for mnist are in './ae_weights/mnist_ae_weights.h5':\n","        python DEC.py mnist --ae_weights ./ae_weights/mnist_ae_weights.h5\n","    for USPS and REUTERSIDF10K datasets\n","        python DEC.py usps --update_interval 30 --ae_weights ./ae_weights/usps_ae_weights.h5\n","        python DEC.py reutersidf10k --n_clusters 4 --update_interval 20 --ae_weights ./ae_weights/reutersidf10k_ae_weights.h5\n","Author:\n","    Xifeng Guo. 2017.1.30\n","\"\"\"\n","\n","from time import time\n","import numpy as np\n","import keras.backend as K\n","from tensorflow.keras.layers import Layer, InputSpec\n","from keras.layers import Dense, Input\n","from keras.models import Model\n","from keras.optimizers import SGD\n","from keras.utils.vis_utils import plot_model\n","import keras\n","\n","from sklearn.cluster import KMeans\n","from sklearn import metrics\n","#from sklearn.utils import linear_assignment\n","from scipy.optimize import linear_sum_assignment as linear_assignment\n","\n","# def cluster_acc(y_true, y_pred):\n","#     \"\"\"\n","#     Calculate clustering accuracy. Require scikit-learn installed\n","#     # Arguments\n","#         y: true labels, numpy.array with shape `(n_samples,)`\n","#         y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n","#     # Return\n","#         accuracy, in [0,1]\n","#     \"\"\"\n","#     y_true = y_true.astype(np.int64)\n","#     assert y_pred.size == y_true.size\n","#     D = max(y_pred.max(), y_true.max()) + 1\n","#     w = np.zeros((D, D), dtype=np.int64)\n","#     for i in range(y_pred.size):\n","#         w[y_pred[i], y_true[i]] += 1\n","#     ind = linear_assignment(w.max() - w)\n","#     return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size\n","\n","def cluster_acc(y_true, y_pred):\n","    \"\"\"\n","    Calculate clustering accuracy. Require scikit-learn installed\n","    # Arguments\n","        y_true: true labels, numpy.array with shape `(n_samples,)`\n","        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n","    # Return\n","        accuracy, in [0,1]\n","    \"\"\"\n","    y_true = y_true.astype(np.int64)\n","    assert y_pred.size == y_true.size\n","    D = max(y_pred.max(), y_true.max()) + 1\n","    w = np.zeros((D, D), dtype=np.int64)\n","    for i in range(y_pred.size):\n","        w[y_pred[i], y_true[i]] += 1\n","    from scipy.optimize import linear_sum_assignment``\n","    ind = linear_sum_assignment(w.max() - w)\n","    return sum([w[i, j] for i, j in zip(ind[0], ind[1])]) * 1.0 / y_pred.size\n","\n","\n","def autoencoder(dims, act='relu'):\n","    \"\"\"\n","    Fully connected auto-encoder model, symmetric.\n","    Arguments:\n","        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n","            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n","        act: activation, not applied to Input, Hidden and Output layers\n","    return:\n","        Model of autoencoder\n","    \"\"\"\n","    n_stacks = len(dims) - 1\n","    # input\n","    x = Input(shape=(dims[0],), name='input')\n","    h = x\n","\n","    # internal layers in encoder\n","    for i in range(n_stacks-1):\n","        h = Dense(dims[i + 1], activation=act, name='encoder_%d' % i)(h)\n","\n","    # hidden layer\n","    h = Dense(dims[-1], name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here\n","\n","    # internal layers in decoder\n","    for i in range(n_stacks-1, 0, -1):\n","        h = Dense(dims[i], activation=act, name='decoder_%d' % i)(h)\n","\n","    # output\n","    h = Dense(dims[0], name='decoder_0')(h)\n","\n","    return Model(inputs=x, outputs=h)\n","\n","\n","class ClusteringLayer(Layer):\n","    \"\"\"\n","    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n","    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n","    # Example\n","    ```\n","        model.add(ClusteringLayer(n_clusters=10))\n","    ```\n","    # Arguments\n","        n_clusters: number of clusters.\n","        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n","        alpha: parameter in Student's t-distribution. Default to 1.0.\n","    # Input shape\n","        2D tensor with shape: `(n_samples, n_features)`.\n","    # Output shape\n","        2D tensor with shape: `(n_samples, n_clusters)`.\n","    \"\"\"\n","\n","    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n","        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n","            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n","        super(ClusteringLayer, self).__init__(**kwargs)\n","        self.n_clusters = int(n_clusters)\n","        self.alpha = alpha\n","        self.initial_weights = weights\n","        self.input_spec = InputSpec(ndim=2)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 2\n","        input_dim = input_shape[1]\n","        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n","        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n","        if self.initial_weights is not None:\n","            self.set_weights(self.initial_weights)\n","            del self.initial_weights\n","        self.built = True\n","\n","    def call(self, inputs, **kwargs):\n","        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n","                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n","        Arguments:\n","            inputs: the variable containing data, shape=(n_samples, n_features)\n","        Return:\n","            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n","        \"\"\"\n","        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n","        q **= (self.alpha + 1.0) / 2.0\n","        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n","        return q\n","\n","    def compute_output_shape(self, input_shape):\n","        assert input_shape and len(input_shape) == 2\n","        return input_shape[0], self.n_clusters\n","\n","    def get_config(self):\n","        config = {'n_clusters': self.n_clusters}\n","        base_config = super(ClusteringLayer, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","class DEC(object):\n","    def __init__(self,\n","                 dims,\n","                 n_clusters=10,\n","                 alpha=1.0,\n","                 batch_size=256):\n","\n","        super(DEC, self).__init__()\n","\n","        self.dims = dims\n","        self.input_dim = dims[0]\n","        self.n_stacks = len(self.dims) - 1\n","\n","        self.n_clusters = n_clusters\n","        self.alpha = alpha\n","        self.batch_size = batch_size\n","        self.autoencoder = autoencoder(self.dims)\n","\n","    def initialize_model(self, optimizer, ae_weights=None):\n","        if ae_weights is not None:  # load pretrained weights of autoencoder\n","            self.autoencoder.load_weights(ae_weights)\n","        # else:\n","            # inputs = Input(shape=(self.input_dim,))\n","            # hidden = inputs\n","            # for i in range(self.n_stacks-1):\n","            #     hidden = Dense(self.dims[i+1], activation='relu')(hidden)\n","            # encoded = Dense(self.dims[-1], activation='linear', name='encoder_%d' % (self.n_stacks-1))(hidden)\n","            # for i in range(self.n_stacks-2, -1, -1):\n","            #     hidden = Dense(self.dims[i], activation='relu')(hidden)\n","            # decoded = Dense(self.input_dim, activation='sigmoid', name='decoder_0')(hidden)\n","            # self.autoencoder = Model(inputs=inputs, outputs=decoded)\n","            # print('ae_weights must be given. E.g.')\n","            # print('python DEC.py mnist --ae_weights weights.h5')\n","            # exit()\n","\n","        hidden = self.autoencoder.get_layer(name='encoder_%d' % (self.n_stacks - 1)).output\n","        self.encoder = Model(inputs=self.autoencoder.input, outputs=hidden)\n","\n","        # prepare DEC model\n","        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(hidden)\n","        self.model = Model(inputs=self.autoencoder.input, outputs=clustering_layer)\n","        self.model.compile(loss='kld', optimizer=optimizer)\n","\n","    def load_weights(self, weights_path):  # load weights of DEC model\n","        self.model.load_weights(weights_path)\n","\n","    def extract_feature(self, x):  # extract features from before clustering layer\n","        encoder = Model(self.model.input, self.model.get_layer('encoder_%d' % (self.n_stacks - 1)).output)\n","        return encoder.predict(x)\n","\n","    def predict_clusters(self, x):  # predict cluster labels using the output of clustering layer\n","        q = self.model.predict(x, verbose=0)\n","        return q.argmax(1)\n","\n","    @staticmethod\n","    def target_distribution(q):\n","        weight = q ** 2 / q.sum(0)\n","        return (weight.T / weight.sum(1)).T\n","\n","    def clustering(self, x, y=None,\n","                   tol=1e-3,\n","                   update_interval=140,\n","                   maxiter=2e4,\n","                   save_dir='./results/dec'):\n","\n","        print('Update interval', update_interval)\n","        save_interval = x.shape[0] / self.batch_size * 5  # 5 epochs\n","        print('Save interval', save_interval)\n","\n","        # initialize cluster centers using k-means\n","        print('Initializing cluster centers with k-means.')\n","        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n","        y_pred = kmeans.fit_predict(self.encoder.predict(x))\n","        y_pred_last = y_pred\n","        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n","\n","        # logging file\n","        import csv, os\n","        if not os.path.exists(save_dir):\n","            os.makedirs(save_dir)\n","\n","        logfile = open(save_dir + '/dec_log.csv', 'w')\n","        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'L'])\n","        logwriter.writeheader()\n","\n","        loss = 0\n","        index = 0\n","        for ite in range(int(maxiter)):\n","            if ite % update_interval == 0:\n","                q = self.model.predict(x, verbose=0)\n","                p = self.target_distribution(q)  # update the auxiliary target distribution p\n","\n","                # evaluate the clustering performance\n","                y_pred = q.argmax(1)\n","                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n","                y_pred_last = y_pred\n","                if y is not None:\n","                    acc = np.round(cluster_acc(y, y_pred), 5)\n","                    nmi = np.round(metrics.normalized_mutual_info_score(y, y_pred), 5)\n","                    ari = np.round(metrics.adjusted_rand_score(y, y_pred), 5)\n","                    loss = np.round(loss, 5)\n","                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, L=loss)\n","                    logwriter.writerow(logdict)\n","                    print('Iter', ite, ': Acc', acc, ', nmi', nmi, ', ari', ari, '; loss=', loss)\n","\n","                # check stop criterion\n","                if ite > 0 and delta_label < tol:\n","                    print('delta_label ', delta_label, '< tol ', tol)\n","                    print('Reached tolerance threshold. Stopping training.')\n","                    logfile.close()\n","                    break\n","\n","            # train on batch\n","            if (index + 1) * self.batch_size > x.shape[0]:\n","                loss = self.model.train_on_batch(x=x[index * self.batch_size::],\n","                                                 y=p[index * self.batch_size::])\n","                index = 0\n","            else:\n","                loss = self.model.train_on_batch(x=x[index * self.batch_size:(index + 1) * self.batch_size],\n","                                                 y=p[index * self.batch_size:(index + 1) * self.batch_size])\n","                index += 1\n","\n","            # save intermediate model\n","            if ite % save_interval == 0:\n","                # save IDEC model checkpoints\n","                print('saving model to:', save_dir + '/DEC_model_' + str(ite) + '.h5')\n","                self.model.save_weights(save_dir + '/DEC_model_' + str(ite) + '.h5')\n","\n","            ite += 1\n","\n","        # save the trained model\n","        logfile.close()\n","        print('saving model to:', save_dir + '/DEC_model_final.h5')\n","        self.model.save_weights(save_dir + '/DEC_model_final.h5')\n","\n","        return y_pred\n"]},{"cell_type":"markdown","metadata":{"id":"T__c64vVKYBC"},"source":["# New Section"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qSPeMi2zMwVA"},"outputs":[],"source":["\"\"\"\n","Implementation for Improved Deep Embedded Clustering as described in paper:\n","        Xifeng Guo, Long Gao, Xinwang Liu, Jianping Yin. Improved Deep Embedded Clustering with Local Structure\n","        Preservation. IJCAI 2017.\n","Usage:\n","    Weights of Pretrained autoencoder for mnist are in './ae_weights/mnist_ae_weights.h5':\n","        python IDEC.py mnist --ae_weights ./ae_weights/mnist_ae_weights.h5\n","    for USPS and REUTERSIDF10K datasets\n","        python IDEC.py usps --update_interval 30 --ae_weights ./ae_weights/usps_ae_weights.h5\n","        python IDEC.py reutersidf10k --n_clusters 4 --update_interval 3 --ae_weights ./ae_weights/reutersidf10k_ae_weights.h5\n","Author:\n","    Xifeng Guo. 2017.4.30\n","\"\"\"\n","\n","from time import time\n","import numpy as np\n","from keras.models import Model\n","from keras.optimizers import SGD\n","from keras.utils.vis_utils import plot_model\n","\n","from sklearn.cluster import KMeans\n","from sklearn import metrics\n","\n","\n","\n","class IDEC(object):\n","    def __init__(self,\n","                 dims,\n","                 n_clusters=10,\n","                 alpha=1.0,\n","                 batch_size=256):\n","\n","        super(IDEC, self).__init__()\n","\n","        self.dims = dims\n","        self.input_dim = dims[0]\n","        self.n_stacks = len(self.dims) - 1\n","\n","        self.n_clusters = n_clusters\n","        self.alpha = alpha\n","        self.batch_size = batch_size\n","        self.autoencoder = autoencoder(self.dims)\n","\n","    def initialize_model(self, ae_weights=None, gamma=0.1, optimizer='adam'):\n","        if ae_weights is not None:\n","            self.autoencoder.load_weights(ae_weights)\n","            print('Pretrained AE weights are loaded successfully.')\n","        # else:\n","        # inputs = Input(shape=(self.input_dim,))\n","        # hidden = inputs\n","        # for i in range(self.n_stacks-1):\n","        #     hidden = Dense(self.dims[i+1], activation='relu')(hidden)\n","        # encoded = Dense(self.dims[-1], activation='linear', name='encoder_%d' % (self.n_stacks-1))(hidden)\n","        # for i in range(self.n_stacks-2, -1, -1):\n","        #     hidden = Dense(self.dims[i], activation='relu')(hidden)\n","        # decoded = Dense(self.input_dim, activation='sigmoid', name='decoder_0')(hidden)\n","        # self.autoencoder = Model(inputs=inputs, outputs=decoded)\n","          # print('ae_weights must be given. E.g.')\n","          # print ('    python IDEC.py mnist --ae_weights weights.h5')\n","          # exit()\n","\n","        hidden = self.autoencoder.get_layer(name='encoder_%d' % (self.n_stacks - 1)).output\n","        self.encoder = Model(inputs=self.autoencoder.input, outputs=hidden)\n","\n","        # prepare IDEC model\n","        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(hidden)\n","        self.model = Model(inputs=self.autoencoder.input,\n","                           outputs=[clustering_layer, self.autoencoder.output])\n","        self.model.compile(loss={'clustering': 'kld', 'decoder_0': 'mse'},\n","                           loss_weights=[gamma, 1],\n","                           optimizer=keras.optimizers.Adam(learning_rate=0.001))\n","\n","    def load_weights(self, weights_path):  # load weights of IDEC model\n","        self.model.load_weights(weights_path)\n","\n","    def extract_feature(self, x):  # extract features from before clustering layer\n","        encoder = Model(self.model.input, self.model.get_layer('encoder_%d' % (self.n_stacks - 1)).output)\n","        return encoder.predict(x)\n","\n","    def predict_clusters(self, x):  # predict cluster labels using the output of clustering layer\n","        q, _ = self.model.predict(x, verbose=0)\n","        return q.argmax(1)\n","\n","    @staticmethod\n","    def target_distribution(q):  # target distribution P which enhances the discrimination of soft label Q\n","        weight = q ** 2 / q.sum(0)\n","        return (weight.T / weight.sum(1)).T\n","\n","    def clustering(self, x, y=None,\n","                   tol=1e-3,\n","                   update_interval=140,\n","                   maxiter=2e4,\n","                   save_dir='./results/idec'):\n","\n","        print ('Update interval', update_interval)\n","        save_interval = x.shape[0] / self.batch_size * 5  # 5 epochs\n","        print('Save interval', save_interval)\n","\n","        # initialize cluster centers using k-means\n","        print('Initializing cluster centers with k-means.')\n","        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n","        y_pred = kmeans.fit_predict(self.encoder.predict(x))\n","        y_pred_last = y_pred\n","        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n","\n","        # logging file\n","        import csv, os\n","        if not os.path.exists(save_dir):\n","            os.makedirs(save_dir)\n","        logfile = open(save_dir + '/idec_log.csv', 'w')\n","        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'L', 'Lc', 'Lr'])\n","        logwriter.writeheader()\n","\n","        loss = [0, 0, 0]\n","        index = 0\n","        for ite in range(int(maxiter)):\n","            if ite % update_interval == 0:\n","                q, _ = self.model.predict(x, verbose=0)\n","                p = self.target_distribution(q)  # update the auxiliary target distribution p\n","\n","                # evaluate the clustering performance\n","                y_pred = q.argmax(1)\n","                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n","                y_pred_last = y_pred\n","                if y is not None:\n","                    acc = np.round(cluster_acc(y, y_pred), 5)\n","                    nmi = np.round(metrics.normalized_mutual_info_score(y, y_pred), 5)\n","                    ari = np.round(metrics.adjusted_rand_score(y, y_pred), 5)\n","                    loss = np.round(loss, 5)\n","                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, L=loss[0], Lc=loss[1], Lr=loss[2])\n","                    logwriter.writerow(logdict)\n","                    # print('Iter', ite, ': Acc', acc, ', nmi', nmi, ', ari', ari, '; loss=', loss)\n","                    print('Iter', ite, ': Acc', acc, '; loss=', loss)\n","\n","\n","                # check stop criterion\n","                if ite > 0 and delta_label < tol:\n","                    print('delta_label ', delta_label, '< tol ', tol)\n","                    print('Reached tolerance threshold. Stopping training.')\n","                    logfile.close()\n","                    break\n","\n","            # train on batch\n","            if (index + 1) * self.batch_size > x.shape[0]:\n","                loss = self.model.train_on_batch(x=x[index * self.batch_size::],\n","                                                 y=[p[index * self.batch_size::], x[index * self.batch_size::]])\n","                index = 0\n","            else:\n","                loss = self.model.train_on_batch(x=x[index * self.batch_size:(index + 1) * self.batch_size],\n","                                                 y=[p[index * self.batch_size:(index + 1) * self.batch_size],\n","                                                    x[index * self.batch_size:(index + 1) * self.batch_size]])\n","                index += 1\n","\n","            # save intermediate model\n","            if ite % save_interval == 0:\n","                # save IDEC model checkpoints\n","                print('saving model to:', save_dir + '/IDEC_model_' + str(ite) + '.h5')\n","                # self.model.save_weights(save_dir + '/IDEC_model_' + str(ite) + '.h5')\n","                self.encoder.save_weights(save_dir + '/IDEC_model_' + str(ite) + '.h5')\n","\n","\n","            ite += 1\n","\n","        # save the trained model\n","        logfile.close()\n","        print('saving model to:', save_dir + '/IDEC_model_final.h5')\n","        self.encoder.save_weights(save_dir + '/IDEC_model_final.h5')\n","\n","        return y_pred"]},{"cell_type":"markdown","metadata":{"id":"WqgQwV39DMhj"},"source":["|H|B|C|\n","|---|---|---|\n","|g|h|h|\n","|g|h|h|\n","|g|h|h|\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-sYij-kM4iD"},"outputs":[],"source":["# import argparse\n","\n","# parser = argparse.ArgumentParser(description='train',\n","#                                  formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n","# parser.add_argument('dataset', default='mnist', choices=['mnist', 'usps', 'reutersidf10k'])\n","# parser.add_argument('--n_clusters', default=10, type=int)\n","# parser.add_argument('--batch_size', default=256, type=int)\n","# parser.add_argument('--maxiter', default=2e4, type=int)\n","# parser.add_argument('--gamma', default=0.1, type=float,\n","#                     help='coefficient of clustering loss')\n","# parser.add_argument('--update_interval', default=140, type=int)\n","# parser.add_argument('--tol', default=0.001, type=float)\n","# parser.add_argument('--ae_weights', default=None, help='This argument must be given')\n","# parser.add_argument('--save_dir', default='results/idec')\n","# args = parser.parse_args()\n","# print(args)\n","\n","\n","dataset =\"mnist\"\n","n_clusters = 2\n","batch_size = 32\n","maxiter = 20000\n","gamma = 0.1\n","update_interval = 140\n","tol = 0.001\n","# ae_weights = \"/content/drive/MyDrive/Colab Notebooks/btp/saved_weights/Copy of IDEC_model_final.h5\"\n","# save_dir = 'results/idec'\n","save_dir=\"/content/drive/MyDrive/Colab Notebooks/btp/results\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3nbP-QrzM9NC"},"outputs":[],"source":["# def load_mnist():\n","#     # the data, shuffled and split between train and test sets\n","#     from keras.datasets import mnist\n","#     (x_train, y_train), (x_test, y_test) = mnist.load_data()\n","#     x = np.concatenate((x_train, x_test))\n","#     y = np.concatenate((y_train, y_test))\n","#     x = x.reshape((x.shape[0], -1))\n","#     x = np.divide(x, 50.)  # normalize as it does in DEC paper\n","#     print ('MNIST samples', x.shape)\n","#     return x,y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AKJdhcP0NFcw"},"outputs":[],"source":["# # load dataset\n","# # optimizer = SGD(lr=0.1, momentum=0.99)\n","# # from datasets import load_mnist, load_reuters, load_usps\n","\n","# if dataset == \"mnist\":  # recommends: n_clusters=10, update_interval=140\n","#     x, y = load_mnist()\n","#     optimizer = 'adam'\n","# # elif dataset == 'usps':  # recommends: n_clusters=10, update_interval=30\n","# #     x, y = load_usps('data/usps')\n","# # elif dataset == 'reutersidf10k':  # recommends: n_clusters=4, update_interval=3\n","# #     x, y = load_reuters('data/reuters')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OxcU2cJ0NOgp"},"outputs":[],"source":["# x, y = load_mnist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZsVH585PPgEY"},"outputs":[],"source":["import scipy.io\n","mat = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/btp/data-and-models-master/postprocessing/ITA_train.mat')\n","mat['ITA_train']\n","import numpy as np\n","data = np.array(mat['ITA_train'])\n","data = np.transpose(data)\n","x = data[:,2:]\n","y = data[:, :1]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vsxOIuXoe2hU"},"outputs":[],"source":["# import scipy.io\n","# mat = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/btp/data-and-models-master/postprocessing/GER_train.mat')\n","# mat['GER_train']\n","# import numpy as np\n","# data = np.array(mat['GER_train'])\n","# data = np.transpose(data)\n","# x = data[:,2:]\n","# y = data[:, :1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1683736561260,"user":{"displayName":"Bhavya Jain","userId":"07476527218486660308"},"user_tz":-330},"id":"Na5pcjkX489O","outputId":"d3e3b2f4-1f9f-4f5b-e645-51c905806f00"},"outputs":[{"data":{"text/plain":["(6981,)"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["y=y.reshape(y.shape[0])\n","y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AL1b3c5DvN7v"},"outputs":[],"source":["# y\n","# freq = [0]*10\n","# count=0\n","# for i in y:\n","#   freq[i]+=1\n","#   count+=1\n","# freq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTTERNeENSpA"},"outputs":[],"source":["optimizer = 'adam'\n","# ae_weights[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":474,"status":"ok","timestamp":1683736564189,"user":{"displayName":"Bhavya Jain","userId":"07476527218486660308"},"user_tz":-330},"id":"FPWj_lg6NY6B","outputId":"9bf24583-4dec-4b6e-aa1c-996e7f905a36"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_14\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input (InputLayer)             [(None, 19)]         0           []                               \n","                                                                                                  \n"," encoder_0 (Dense)              (None, 200)          4000        ['input[0][0]']                  \n","                                                                                                  \n"," encoder_1 (Dense)              (None, 500)          100500      ['encoder_0[0][0]']              \n","                                                                                                  \n"," encoder_2 (Dense)              (None, 2000)         1002000     ['encoder_1[0][0]']              \n","                                                                                                  \n"," encoder_3 (Dense)              (None, 10)           20010       ['encoder_2[0][0]']              \n","                                                                                                  \n"," decoder_3 (Dense)              (None, 2000)         22000       ['encoder_3[0][0]']              \n","                                                                                                  \n"," decoder_2 (Dense)              (None, 500)          1000500     ['decoder_3[0][0]']              \n","                                                                                                  \n"," decoder_1 (Dense)              (None, 200)          100200      ['decoder_2[0][0]']              \n","                                                                                                  \n"," clustering (ClusteringLayer)   (None, 2)            20          ['encoder_3[0][0]']              \n","                                                                                                  \n"," decoder_0 (Dense)              (None, 19)           3819        ['decoder_1[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 2,253,049\n","Trainable params: 2,253,049\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["# prepare the IDEC model\n","idec = IDEC(dims=[x.shape[-1], 200, 500, 2000, 10], n_clusters=n_clusters, batch_size=batch_size)\n","idec.initialize_model(gamma=gamma, optimizer=optimizer)\n","plot_model(idec.model, to_file='idec_model.png', show_shapes=True)\n","idec.model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":794653,"status":"ok","timestamp":1683737358836,"user":{"displayName":"Bhavya Jain","userId":"07476527218486660308"},"user_tz":-330},"id":"CfRBNgwhNe4T","outputId":"f78d921d-5743-472c-9aeb-1799d96e6566"},"outputs":[{"name":"stdout","output_type":"stream","text":["Update interval 140\n","Save interval 1603.4375\n","Initializing cluster centers with k-means.\n","321/321 [==============================] - 1s 4ms/step\n","Iter 0 : Acc 0.60437 ; loss= [0 0 0]\n","saving model to: /content/drive/MyDrive/Colab Notebooks/btp/results/IDEC_model_0.h5\n","Iter 140 : Acc 0.61937 ; loss= [0.02994 0.00053 0.02989]\n","Iter 280 : Acc 0.62795 ; loss= [0.01958 0.00011 0.01956]\n","Iter 420 : Acc 0.64188 ; loss= [0.01096 0.00017 0.01094]\n","Iter 560 : Acc 0.64081 ; loss= [0.00637 0.00012 0.00636]\n","Iter 700 : Acc 0.63984 ; loss= [0.00476 0.0006  0.0047 ]\n","Iter 840 : Acc 0.63964 ; loss= [0.00417 0.0004  0.00413]\n","Iter 980 : Acc 0.63837 ; loss= [0.00618 0.00325 0.00585]\n","Iter 1120 : Acc 0.63721 ; loss= [0.00975 0.00987 0.00877]\n","Iter 1260 : Acc 0.63818 ; loss= [0.00879 0.02138 0.00665]\n","Iter 1400 : Acc 0.63837 ; loss= [0.00805 0.02424 0.00563]\n","Iter 1540 : Acc 0.63925 ; loss= [0.02623 0.04115 0.02212]\n","Iter 1680 : Acc 0.63857 ; loss= [0.00573 0.0279  0.00294]\n","Iter 1820 : Acc 0.63818 ; loss= [0.00683 0.02629 0.0042 ]\n","Iter 1960 : Acc 0.63876 ; loss= [0.00728 0.02809 0.00447]\n","Iter 2100 : Acc 0.63808 ; loss= [0.01664 0.02497 0.01414]\n","Iter 2240 : Acc 0.63789 ; loss= [0.01366 0.02989 0.01067]\n","Iter 2380 : Acc 0.6374 ; loss= [0.00767 0.02896 0.00477]\n","Iter 2520 : Acc 0.63565 ; loss= [0.01284 0.02308 0.01053]\n","Iter 2660 : Acc 0.63682 ; loss= [0.00657 0.02392 0.00418]\n","Iter 2800 : Acc 0.63691 ; loss= [0.00583 0.02223 0.00361]\n","Iter 2940 : Acc 0.63808 ; loss= [0.00581 0.02391 0.00342]\n","Iter 3080 : Acc 0.6373 ; loss= [0.00594 0.01908 0.00403]\n","Iter 3220 : Acc 0.63876 ; loss= [0.01056 0.04663 0.0059 ]\n","Iter 3360 : Acc 0.63857 ; loss= [0.00874 0.02863 0.00587]\n","Iter 3500 : Acc 0.63828 ; loss= [0.00878 0.0441  0.00437]\n","Iter 3640 : Acc 0.6375 ; loss= [0.0121  0.02204 0.00989]\n","Iter 3780 : Acc 0.63896 ; loss= [0.00857 0.02031 0.00654]\n","Iter 3920 : Acc 0.63876 ; loss= [0.00661 0.01952 0.00466]\n","Iter 4060 : Acc 0.63876 ; loss= [0.00558 0.02557 0.00303]\n","Iter 4200 : Acc 0.63769 ; loss= [0.00857 0.05713 0.00286]\n","Iter 4340 : Acc 0.63837 ; loss= [0.00655 0.0216  0.00439]\n","Iter 4480 : Acc 0.63984 ; loss= [0.00606 0.01622 0.00444]\n","Iter 4620 : Acc 0.63886 ; loss= [0.00424 0.01708 0.00253]\n","Iter 4760 : Acc 0.63818 ; loss= [0.01277 0.04997 0.00778]\n","Iter 4900 : Acc 0.63984 ; loss= [0.00368 0.01482 0.0022 ]\n","Iter 5040 : Acc 0.64062 ; loss= [0.01038 0.03019 0.00736]\n","Iter 5180 : Acc 0.64383 ; loss= [0.00454 0.01418 0.00312]\n","Iter 5320 : Acc 0.64286 ; loss= [0.0038  0.01636 0.00217]\n","Iter 5460 : Acc 0.64081 ; loss= [0.01781 0.14736 0.00308]\n","Iter 5600 : Acc 0.64042 ; loss= [0.00516 0.01634 0.00352]\n","Iter 5740 : Acc 0.63993 ; loss= [0.00913 0.04417 0.00472]\n","Iter 5880 : Acc 0.6413 ; loss= [0.00393 0.01267 0.00266]\n","Iter 6020 : Acc 0.64169 ; loss= [0.0049  0.01298 0.00361]\n","Iter 6160 : Acc 0.64247 ; loss= [0.0045  0.0238  0.00212]\n","Iter 6300 : Acc 0.64208 ; loss= [0.00369 0.01156 0.00253]\n","Iter 6440 : Acc 0.64237 ; loss= [0.005   0.01571 0.00343]\n","Iter 6580 : Acc 0.64179 ; loss= [0.00473 0.01391 0.00334]\n","Iter 6720 : Acc 0.64256 ; loss= [0.0036  0.01435 0.00217]\n","Iter 6860 : Acc 0.63769 ; loss= [0.00416 0.01134 0.00302]\n","Iter 7000 : Acc 0.63857 ; loss= [0.00725 0.01218 0.00603]\n","Iter 7140 : Acc 0.64188 ; loss= [0.01009 0.0794  0.00215]\n","Iter 7280 : Acc 0.6414 ; loss= [0.00414 0.01297 0.00284]\n","Iter 7420 : Acc 0.64081 ; loss= [0.00358 0.0193  0.00165]\n","Iter 7560 : Acc 0.64081 ; loss= [0.00576 0.01939 0.00382]\n","Iter 7700 : Acc 0.63974 ; loss= [0.00422 0.01118 0.0031 ]\n","Iter 7840 : Acc 0.63935 ; loss= [0.00506 0.01273 0.00378]\n","Iter 7980 : Acc 0.6411 ; loss= [0.0057  0.01215 0.00448]\n","Iter 8120 : Acc 0.64062 ; loss= [0.00284 0.01111 0.00173]\n","Iter 8260 : Acc 0.64062 ; loss= [0.00433 0.01293 0.00304]\n","Iter 8400 : Acc 0.6413 ; loss= [0.0032  0.01043 0.00216]\n","Iter 8540 : Acc 0.64052 ; loss= [0.00422 0.01088 0.00313]\n","Iter 8680 : Acc 0.64023 ; loss= [0.00334 0.01029 0.00231]\n","Iter 8820 : Acc 0.63837 ; loss= [0.01024 0.04202 0.00603]\n","Iter 8960 : Acc 0.6373 ; loss= [0.00198 0.00791 0.00119]\n","Iter 9100 : Acc 0.63779 ; loss= [0.00799 0.03513 0.00448]\n","Iter 9240 : Acc 0.63721 ; loss= [0.00372 0.01001 0.00272]\n","Iter 9380 : Acc 0.63867 ; loss= [0.00661 0.02327 0.00428]\n","Iter 9520 : Acc 0.63652 ; loss= [0.00417 0.01949 0.00222]\n","Iter 9660 : Acc 0.63633 ; loss= [0.0035  0.01567 0.00193]\n","Iter 9800 : Acc 0.63613 ; loss= [0.00556 0.01164 0.0044 ]\n","Iter 9940 : Acc 0.63146 ; loss= [0.0044  0.01075 0.00333]\n","Iter 10080 : Acc 0.63116 ; loss= [0.00223 0.00831 0.0014 ]\n","Iter 10220 : Acc 0.62931 ; loss= [0.00429 0.01069 0.00322]\n","Iter 10360 : Acc 0.62873 ; loss= [0.00365 0.0144  0.00221]\n","Iter 10500 : Acc 0.6299 ; loss= [0.0039  0.02009 0.00189]\n","Iter 10640 : Acc 0.62766 ; loss= [0.00343 0.01047 0.00239]\n","Iter 10780 : Acc 0.62766 ; loss= [0.00547 0.02342 0.00313]\n","Iter 10920 : Acc 0.62736 ; loss= [0.00977 0.03158 0.00661]\n","Iter 11060 : Acc 0.63058 ; loss= [0.00357 0.00851 0.00271]\n","Iter 11200 : Acc 0.626 ; loss= [0.00429 0.02053 0.00224]\n","Iter 11340 : Acc 0.62619 ; loss= [0.00349 0.01593 0.00189]\n","Iter 11480 : Acc 0.62736 ; loss= [0.00588 0.01007 0.00487]\n","Iter 11620 : Acc 0.6258 ; loss= [0.0036  0.0199  0.00161]\n","Iter 11760 : Acc 0.62551 ; loss= [0.00368 0.02369 0.00131]\n","Iter 11900 : Acc 0.62619 ; loss= [0.00498 0.02284 0.00269]\n","Iter 12040 : Acc 0.62473 ; loss= [0.00347 0.00842 0.00262]\n","Iter 12180 : Acc 0.62551 ; loss= [0.00493 0.02083 0.00285]\n","Iter 12320 : Acc 0.62541 ; loss= [0.00337 0.012   0.00217]\n","Iter 12460 : Acc 0.62668 ; loss= [0.00544 0.02355 0.00309]\n","Iter 12600 : Acc 0.62853 ; loss= [0.00248 0.00764 0.00171]\n","Iter 12740 : Acc 0.63185 ; loss= [0.00494 0.02103 0.00284]\n","Iter 12880 : Acc 0.63087 ; loss= [0.00244 0.00954 0.00148]\n","Iter 13020 : Acc 0.62795 ; loss= [0.00396 0.01815 0.00214]\n","Iter 13160 : Acc 0.6259 ; loss= [0.00327 0.0098  0.00229]\n","Iter 13300 : Acc 0.62493 ; loss= [0.01012 0.06932 0.00319]\n","Iter 13440 : Acc 0.62912 ; loss= [0.00311 0.00821 0.00229]\n","Iter 13580 : Acc 0.63126 ; loss= [0.0042  0.02454 0.00175]\n","Iter 13720 : Acc 0.62619 ; loss= [0.00441 0.01943 0.00247]\n","Iter 13860 : Acc 0.62873 ; loss= [0.00281 0.0104  0.00177]\n","Iter 14000 : Acc 0.63185 ; loss= [0.00479 0.01974 0.00282]\n","Iter 14140 : Acc 0.63224 ; loss= [0.0038  0.0172  0.00208]\n","Iter 14280 : Acc 0.6336 ; loss= [0.00416 0.01016 0.00315]\n","Iter 14420 : Acc 0.63331 ; loss= [0.00251 0.00744 0.00176]\n","Iter 14560 : Acc 0.6335 ; loss= [0.00584 0.02831 0.00301]\n","Iter 14700 : Acc 0.63311 ; loss= [0.00437 0.01394 0.00298]\n","Iter 14840 : Acc 0.63224 ; loss= [0.00343 0.0151  0.00192]\n","Iter 14980 : Acc 0.63204 ; loss= [0.00288 0.00937 0.00194]\n","Iter 15120 : Acc 0.63136 ; loss= [0.00247 0.00793 0.00167]\n","Iter 15260 : Acc 0.63097 ; loss= [0.00955 0.00748 0.0088 ]\n","Iter 15400 : Acc 0.63126 ; loss= [0.00261 0.00692 0.00192]\n","Iter 15540 : Acc 0.63097 ; loss= [0.00292 0.00835 0.00209]\n","Iter 15680 : Acc 0.63107 ; loss= [0.00218 0.00719 0.00146]\n","Iter 15820 : Acc 0.63107 ; loss= [0.00358 0.01461 0.00212]\n","delta_label  0.0007795751315533034 < tol  0.001\n","Reached tolerance threshold. Stopping training.\n","saving model to: /content/drive/MyDrive/Colab Notebooks/btp/results/IDEC_model_final.h5\n","acc: 0.6310660689923991\n","clustering time:  996.4570586681366\n"]}],"source":["# begin clustering, time not include pretraining part.\n","t0 = time()\n","y_pred = idec.clustering(x, y=y, tol=tol, maxiter=maxiter,\n","                         update_interval=update_interval, save_dir=\"/content/drive/MyDrive/Colab Notebooks/btp/results\")\n","print ('acc:', cluster_acc(y, y_pred))\n","print ('clustering time: ', (time() - t0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yWvj_rlJtjkV"},"outputs":[],"source":["# unique_classes = np.unique(y_pred)\n","# class_counts = {c: np.count_nonzero(y_pred == c) for c in unique_classes}\n","\n","# for c in class_counts:\n","#     print(f\"Class {c}: {class_counts[c]}\")"]},{"cell_type":"markdown","metadata":{"id":"hiwoCJOtyhSw"},"source":["# New Section"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qM6R6Smqw_eY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N26mzb0LP9Ku"},"outputs":[],"source":["# freq = [0]*10\n","# count=0\n","# for i in y:\n","#   freq[i]+=1\n","#   count+=1\n","# freq\n","# # count"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1681468663813,"user":{"displayName":"Bhavya Jain","userId":"07476527218486660308"},"user_tz":-330},"id":"NrDusoL943d0","outputId":"755aaa62-c4a2-433c-c036-6453e8337b86"},"outputs":[{"data":{"text/plain":["array([[0.17288867, 0.16044151, 0.10880093, ..., 0.39100349, 0.42424242,\n","        0.46551724],\n","       [0.06539222, 0.07999692, 0.06475227, ..., 0.17344816, 0.57575758,\n","        0.53448276],\n","       [0.15515482, 0.3123767 , 0.18473457, ..., 0.63972079, 0.73333333,\n","        0.77142857],\n","       ...,\n","       [0.23718126, 0.22727425, 0.18765866, ..., 0.43851575, 0.44444444,\n","        0.43478261],\n","       [0.10487369, 0.11139848, 0.10635577, ..., 0.11059878, 0.14814815,\n","        0.26086957],\n","       [0.12130756, 0.122106  , 0.1203688 , ..., 0.23272099, 0.40740741,\n","        0.30434783]])"]},"execution_count":113,"metadata":{},"output_type":"execute_result"}],"source":["x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"elchOViQ7scP"},"outputs":[],"source":["# modl = IDEC(dims=[x.shape[-1], 500, 500, 2000, 10], n_clusters=n_clusters, batch_size=batch_size)\n","# modl.initialize_model(ae_weights=ae_weights, gamma=gamma, optimizer=optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tCnDLXT9uRqq"},"outputs":[],"source":["# from tensorflow.keras.models import load_model\n","# modl.load_weights('/content/drive/MyDrive/Colab Notebooks/btp/results/IDEC_model_final.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U11_7xDb-aBP"},"outputs":[],"source":["# modl.model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-WEFYpv9s7x"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aeIK1Q6y88Aj"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uMYHyuWJ74TH"},"outputs":[],"source":["# ny =predict_with_idc(model, x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-MGYBfVs3jAv"},"outputs":[],"source":["# import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r342Z_kuzNAo"},"outputs":[],"source":["# ny = model.predict_on_batch(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QKP6NAdc1erF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFymOong2aqj"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sxh1LU9rNTZz"},"outputs":[],"source":["# import scipy.io\n","# mat = scipy.io.loadmat('/content/GER_train.mat')\n","# mat['GER_train']\n","\n","# import numpy as np\n","# data = np.array(mat['GER_train'])\n","# data = np.transpose(data)\n","# X = data[:,2:]\n","# Y = data[:, :1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZE7Z-5NNhAt"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92RXw8kSbP_k"},"outputs":[],"source":["\n","\n",""]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1ixuD0_-nfOVkrhjrxp_b39uWn_rnb5Yw","authorship_tag":"ABX9TyNbLRhQ9u1hxXpUDlPMLtfl"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}